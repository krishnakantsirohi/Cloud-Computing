#!/bin/bash
#SBATCH -A uot143
#SBATCH --job-name="LastFM"
#SBATCH --output="LastFM.distr.out"
#SBATCH --partition=compute
## allocate 3 nodes for the Hadoop cluster: 3 datanodes, from which 1 is namenode
#SBATCH --nodes=3
#SBATCH --ntasks-per-node=1
#SBATCH --mem=5G
#SBATCH --export=ALL
#SBATCH --time=60
export HADOOP_CONF_DIR=/home/$USER/cometcluster
module load hadoop/2.6.0 spark/1.5.2
myhadoop-configure.sh
source /home/$USER/cometcluster/spark/spark-env.sh
start-dfs.sh
start-yarn.sh
myspark start
hdfs dfs -mkdir -p /user/$USER
hdfs dfs -put user_artists.dat /user/$USER/user_artists.dat
hdfs dfs -put user_taggedartists.dat /user/$USER/user_taggedartists.dat
hdfs dfs -put user_friends.dat /user/$USER/user_friends.dat
spark-submit --class LastFM --num-executors 2 LastFM.jar /user/$USER/user_artists.dat /user/$USER/user_taggedartists.dat /user/$USER/user_friends.dat output-distr
stop-yarn.sh
stop-dfs.sh
myhadoop-cleanup.sh